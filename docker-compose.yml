services:
  api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - OLLAMA_MODEL=${OLLAMA_MODEL:-llama3.2}
      - OLLAMA_JUDGE_MODEL=${OLLAMA_JUDGE_MODEL:-mistral}
      - OLLAMA_EMBED_MODEL=${OLLAMA_EMBED_MODEL:-nomic-embed-text}
      - MLFLOW_TRACKING_URI=${MLFLOW_TRACKING_URI:-http://mlflow:5000}
      - OLLAMA_BASE_URL=http://ollama:11434
      - OLLAMA_API_BASE=http://ollama:11434
      - CHROMA_PERSIST_DIR=/chroma/data
    volumes:
      - chroma_data:/chroma/data
    depends_on:
      - ollama
      - mlflow
    restart: on-failure

  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama

  mlflow:
    image: ghcr.io/mlflow/mlflow:latest
    ports:
      - "5000:5000"
    volumes:
      - ./mlflow/data:/mlflow/data
      - ./mlflow/artifacts:/mlflow/artifacts
    command: >
      mlflow server
      --host 0.0.0.0
      --port 5000
      --backend-store-uri sqlite:////mlflow/data/mlflow.db
      --default-artifact-root /mlflow/artifacts
      --allowed-hosts "*"
      --cors-allowed-origins "http://0.0.0.0:5000,http://localhost:5000"

volumes:
  ollama_data:
  chroma_data:
