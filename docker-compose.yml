x-ollama: &service-ollama
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    ports:
      - 11434:11434
    volumes:
        - ollama_data:/root/.ollama

services:
  api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - OLLAMA_MODEL=${OLLAMA_MODEL:-llama3.2}
      - OLLAMA_JUDGE_MODEL=${OLLAMA_JUDGE_MODEL:-mistral}
      - OLLAMA_EMBED_MODEL=${OLLAMA_EMBED_MODEL:-nomic-embed-text}
      - MLFLOW_TRACKING_URI=${MLFLOW_TRACKING_URI:-http://mlflow:5000}
      - OLLAMA_BASE_URL=http://ollama:11434
      - OLLAMA_API_BASE=http://ollama:11434
      - CHROMA_PERSIST_DIR=/chroma/data
    volumes:
      - chroma_data:/chroma/data
    depends_on:
      ollama-cpu:
        condition: service_started
        required: false
      ollama-gpu-amd:
        condition: service_started
        required: false
      ollama-gpu-nvidia:
        condition: service_started
        required: false
      mlflow:
        condition: service_started
    restart: on-failure

  mlflow:
    image: ghcr.io/mlflow/mlflow:latest
    ports:
      - "5000:5000"
    volumes:
      - ./mlflow/data:/mlflow/data
      - ./mlflow/artifacts:/mlflow/artifacts
    command: >
      mlflow server
      --host 0.0.0.0
      --port 5000
      --backend-store-uri sqlite:////mlflow/data/mlflow.db
      --default-artifact-root /mlflow/artifacts
      --allowed-hosts "*"
      --cors-allowed-origins "http://0.0.0.0:5000,http://localhost:5000"

  ollama-cpu:
      profiles: ["cpu"]
      <<: *service-ollama

  ollama-gpu-amd:
      profiles: ["gpu-amd"]
      <<: *service-ollama
      image: ollama/ollama:rocm
      devices:
        - "/dev/kfd"
        - "/dev/dri"

  ollama-gpu-nvidia:
      profiles: ["gpu-nvidia"]
      <<: *service-ollama
      deploy:
        resources:
          reservations:
            devices:
              - driver: nvidia
                count: 1
                capabilities: [gpu]

volumes:
  ollama_data:
  chroma_data:
