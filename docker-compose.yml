services:
  api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - OLLAMA_MODEL=${OLLAMA_MODEL:-llama3.2}
      - OLLAMA_JUDGE_MODEL=${OLLAMA_JUDGE_MODEL:-mistral}
      - OLLAMA_EMBED_MODEL=${OLLAMA_EMBED_MODEL:-nomic-embed-text}
      - CHROMA_HOST=${CHROMA_HOST:-chromadb}
      - CHROMA_PORT=${CHROMA_PORT:-8000}
      - MLFLOW_TRACKING_URI=${MLFLOW_TRACKING_URI:-http://mlflow:5000}
      - OLLAMA_BASE_URL=http://ollama:11434
    depends_on:
      - chromadb
      - ollama
      - mlflow
    restart: on-failure

  chromadb:
    image: chromadb/chroma:0.5.15
    ports:
      - "8001:8000"
    volumes:
      - chromadb_data:/chroma/chroma

  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama

  mlflow:
    image: ghcr.io/mlflow/mlflow:latest
    ports:
      - "5000:5000"
    volumes:
      - ./mlflow/data:/mlflow/data
      - ./mlflow/artifacts:/mlflow/artifacts
    command: >
      mlflow server
      --host 0.0.0.0
      --port 5000
      --backend-store-uri sqlite:////mlflow/data/mlflow.db
      --default-artifact-root /mlflow/artifacts
      --allowed-hosts "*"
      --cors-allowed-origins "http://0.0.0.0:5000,http://localhost:5000"

volumes:
  chromadb_data:
  ollama_data:
